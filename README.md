# Awesome-Video-Generation [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
> Topics about: <br>
> `Text-to-Image`, `Text-to-Video`, `Image-to-Video`, `Video-to-Video`

**This project is curated and maintained by [Rui Sun](https://github.com/ray-ruisun) and [Yumin Zhang](https://github.com/zymvszym).**

## Table of Content
* [Text-to-Image](#text_to_image)
* [Text-to-Video](#text_to_video)
* [Image-to-Video](#image_to_video)
* [Video-to-Video](#video_to_video)

## <a name="text_to_image"></a> Text-to-Image
- **Scalable Diffusion Models with Transformers** `Sequential Images` <br>
  Team: UC Berkeley, NYU. <br>
  *William Peebles, Saining Xie* <br>
  **ICCV'23(Oral)**, arXiv, 2022.12 [[Paper](https://arxiv.org/abs/2212.09748)], [[PDF](https://arxiv.org/pdf/2212.09748.pdf)], [[Code](https://github.com/facebookresearch/DiT)], [[Pretrained Model](https://github.com/facebookresearch/DiT)], [[Home Page](https://www.wpeebles.com/DiT.html)] <br>

## <a name="text_to_video"></a> Text-to-Video
- **World Model on Million-Length Video And Language With RingAttention** `Long Video` <br>
  Team: UC Berkeley. <br>
  *Hao Liu, Wilson Yan, Matei Zaharia, Pieter Abbeel* <br>
  arXiv, 2024.02 [[Paper](https://arxiv.org/abs/2402.08268)], [[PDF](https://arxiv.org/pdf/2402.08268.pdf)], [[Code](https://github.com/LargeWorldModel/LWM)], [[Pretrained Model](https://huggingface.co/LargeWorldModel)], [[Home Page](https://largeworldmodel.github.io/)] <br>
- **360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model** <br>
  Team: Peking University. <br>
  *Qian Wang, Weiqi Li, Chong Mou, et al., Jian Zhang* <br>
  arXiv, 2024.01 [[Paper](https://arxiv.org/abs/2401.04468)], [[PDF](https://arxiv.org/pdf/2401.04468.pdf)], [[Code](https://github.com/Akaneqwq/360DVD)], [[Home Page](https://akaneqwq.github.io/360DVD/)] <br>
- **MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation** <br>
  Team: Bytedance Inc. <br>
  *Weimin Wang, Jiawei Liu, Zhijie Lin, et al., Jiashi Feng* <br>
  arXiv, 2024.01 [[Paper](https://arxiv.org/abs/2401.04468)], [[PDF](https://arxiv.org/pdf/2401.04468.pdf)], [[Home Page](https://magicvideov2.github.io)] <br>
- **UniVG: Towards UNIfied-modal Video Generation** <br>
  Team: Baidu Inc. <br>
  *Ludan Ruan, Lei Tian, Chuanwei Huang, et al., Xinyan Xiao* <br>
  arXiv, 2024.01 [[Paper](https://arxiv.org/abs/2401.09084)], [[PDF](https://arxiv.org/pdf/2401.09084.pdf)], [[Home Page](https://univg-baidu.github.io)] <br>
- **VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM**
  Team: HiDream.ai Inc. <br>
  *Fuchen Long, Zhaofan Qiu, Ting Yao and Tao Mei* <br>
  arXiv, 2024.01 [[Paper](https://arxiv.org/abs/2401.01256)], [[PDF](https://arxiv.org/pdf/2401.01256.pdf)], [[Home Page](https://videodrafter.github.io)] <br>
- **VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models** <br>
  Team: Tencent AI Lab. <br>
  *Haoxin Chen, Yong Zhang, Xiaodong Cun, et al., Ying Shan* <br>
  arXiv, 2024.01 [[Paper](https://arxiv.org/abs/2401.09047)], [[PDF](https://arxiv.org/pdf/2401.09047.pdf)], [[Code](https://github.com/AILab-CVC/VideoCrafter?tab=readme-ov-file)], [[Pretrained Model](https://github.com/AILab-CVC/VideoCrafter?tab=readme-ov-file)], [[Home Page](https://ailab-cvc.github.io/videocrafter2/)] <br>
- **Lumiere: A Space-Time Diffusion Model for Video Generation** <br>
  Team: Google Research, Weizmann Institute, Tel-Aviv University, Technion. <br>
  *Omer Bar-Tal, Hila Chefer, Omer Tov, et al., Inbar Mosseri* <br>
  arXiv, 2024.01 [[Paper](https://arxiv.org/abs/2401.12945)], [[PDF](https://arxiv.org/pdf/2401.12945.pdf)], [[Home Page](https://lumiere-video.github.io/)] <br>
- **VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation** <br>
  Team: Peking University, Microsoft Research. <br>
  *Wenjing Wang, Huan Yang, Zixi Tuo, et al., Jiaying Liu* <br>
  arxiv, 2023.12 [[Paper](https://arxiv.org/abs/2312.14125)], [[PDF](https://arxiv.org/pdf/2312.14125.pdf)] <br>
- **A Recipe for Scaling up Text-to-Video Generation with Text-free Videos** <br>
  Team: HUST, Alibaba Group, Zhejiang University, Ant Group <br>
  *Xiang Wang, Shiwei Zhang, Hangjie Yuan, et al., Nong Sang* <br>
  arxiv, 2023.12 [[Paper](https://arxiv.org/abs/2312.12490)], [[PDF](https://arxiv.org/pdf/2312.12490.pdf)], [[Code](https://github.com/ali-vilab/i2vgen-xl)], [[Home Page](https://instructvideo.github.io/)]<br>
- **InstructVideo: Instructing Video Diffusion Models with Human Feedback** <br>
  Team: Zhejiang University, Alibaba Group, Tsinghua University <br>
  *Hangjie Yuan, Shiwei Zhang, Xiang Wang, et al., Dong Ni* <br>
  arxiv, 2023.12 [[Paper](https://arxiv.org/pdf/2312.09109.pdf)], [[PDF](https://arxiv.org/pdf/2312.15770.pdf)], [[Code](https://github.com/ali-vilab/i2vgen-xl/blob/main/doc/InstructVideo.md)], [[Home Page](https://tf-t2v.github.io)]<br>
- **VideoLCM: Video Latent Consistency Model** <br>
  Team: HUST, Alibaba Group, SJTU <br>
  *Xiang Wang, Shiwei Zhang, Han Zhang, et al., Nong Sang* <br>
  arxiv, 2023.12 [[Paper](https://arxiv.org/abs/2312.09109#:~:text=VideoLCM%20builds%20upon%20existing%20latent,efficiency%2C%20fidelity%20and%20temporal%20consistency.)], [[PDF](https://arxiv.org/pdf/2312.09109.pdf)], [[Code](https://github.com/ali-vilab/i2vgen-xl/blob/main/doc/InstructVideo.md)], [[Home Page](https://tf-t2v.github.io)]<br>
- **Photorealistic Video Generation with Diffusion Models** <br>
  Team: Stanford University Fei-Fei Li, Google. <br>
  *Agrim Gupta, Lijun Yu, Kihyuk Sohn, et al., José Lezama* <br>
  arXiv, 2023.12 [[Paper](https://arxiv.org/abs/2312.06662)], [[PDF](https://arxiv.org/pdf/2312.06662.pdf)], [[Home Page](https://walt-video-diffusion.github.io/)] <br>
<<<<<<< Updated upstream
- **ART⋅V: Auto-Regressive Text-to-Video Generation with Diffusion Models** <br>
  Team: University of Science and Technology of China, Microsoft. <br>
  *Wenming Weng, Ruoyu Feng, Yanhui Wang, et al., Zhiwei Xiong* <br>
  arXiv, 2023.11 [[Paper](https://arxiv.org/abs/2311.18834)], [[PDF](https://arxiv.org/pdf/2311.18834.pdf)], [[Code(coming)](https://github.com/WarranWeng/ART.V)], [[Home Page](https://warranweng.github.io/art.v/)], [[Demo(video)](https://www.youtube.com/watch?v=KX-BV7a-S0Q)] <br>
- **Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets** <br>
  Team: Stability AI. <br>
  *Andreas Blattmann, Tim Dockhorn, Sumith Kulal, et al., Robin Rombach* <br>
  arXiv, 2023.11 [[Paper](https://arxiv.org/abs/2311.15127)], [[PDF](https://arxiv.org/pdf/2311.15127.pdf)], [[Code](https://github.com/Stability-AI/generative-models)] <br>
- **FusionFrames: Efficient Architectural Aspects for Text-to-Video Generation Pipeline** <br>
  Team: Sber AI. <br>
  *Vladimir Arkhipkin, Zein Shaheen, Viacheslav Vasilev, et al., Denis Dimitrov* <br>
  arXiv, 2023.11 [[Paper](https://arxiv.org/abs/2311.13073)], [[PDF](https://arxiv.org/pdf/2311.13073.pdf)], [[Code](https://github.com/ai-forever/KandinskyVideo)], [[Home Page](hhttps://ai-forever.github.io/kandinsky-video/)], [[Demo(live)](https://replicate.com/cjwbw/kandinskyvideo)] <br>
- **MoVideo: Motion-Aware Video Generation with Diffusion Models** <br>
  Team: ETH, Meta. <br>
  *Jingyun Liang, Yuchen Fan, Kai Zhang, et al., Rakesh Ranjan* <br>
  arXiv, 2023.11 [[Paper](https://arxiv.org/abs/2311.11325)], [[PDF](https://arxiv.org/pdf/2311.11325.pdf)], [[Home Page](https://jingyunliang.github.io/MoVideo/)] <br>
- **Optimal Noise pursuit for Augmenting Text-to-Video Generation** <br>
  Team: Zhejiang Lab. <br>
  *Shijie Ma, Huayi Xu, Mengjian Li, et al., Yaxiong Wang* <br>
  arXiv, 2023.11 [[Paper](https://arxiv.org/abs/2311.00949)], [[PDF](https://arxiv.org/pdf/2311.00949.pdf)] <br>
=======
- **Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation** <br>
  Team: HUST, Alibaba Group, Fudan University. <br>
  *Zhiwu Qing, Shiwei Zhang, Jiayu Wang, et al., Nong Sang* <br>
  arXiv, 2023.12 [[Paper](https://arxiv.org/abs/2312.06662)], [[PDF](https://arxiv.org/pdf/2312.06662.pdf)], [[Code](https://github.com/ali-vilab/i2vgen-xl)], [[Pretrained Model](https://huggingface.co/spaces/damo-vilab/I2VGen-XL)], [[Home Page](https://higen-t2v.github.io)] <br>
- **GenTron: Delving Deep into Diffusion Transformers for Image and Video Generation** <br>
  Team: HKU, Meta. <br>
  *Shoufa Chen, Mengmeng Xu, Jiawei Ren, et al., Sen He* <br>
  arXiv, 2023.12 [[Paper](https://arxiv.org/abs/2312.04557)], [[PDF](https://arxiv.org/pdf/2312.04557.pdf)], [[Home Page](https://www.shoufachen.com/gentron_website/)] <br>
- **StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter** <br>
  Team: Tsinghua University, Tencent AI Lab, CUHK. <br>
  *Gongye Liu, Menghan Xia, Yong Zhang, et al., Ying Shan* <br>
  arXiv, 2023.12 [[Paper](https://arxiv.org/abs/2312.00330)], [[PDF](https://arxiv.org/pdf/2312.00330.pdf)], [[Code](https://gongyeliu.github.io/StyleCrafter.github.io/)], [[Home Page](https://gongyeliu.github.io/StyleCrafter.github.io/)], [[Demo](https://huggingface.co/spaces/liuhuohuo/StyleCrafter)] <br>
- **MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation** <br>
  Team: University of Science and Technology of China, MSRA, Xi'an Jiaotong University. <br>
  *Yanhui Wang, Jianmin Bao, Wenming Weng, et al., Baining Guo* <br>
  arXiv, 2023.11 [[Paper](https://arxiv.org/abs/2311.18829#:~:text=Unlike%20existing%20approaches%20that%20align,%26text%2Dto%2Dvideo%20generation.)], [[PDF](https://arxiv.org/pdf/2311.18829.pdf)], [[Home Page](https://wangyanhui666.github.io/MicroCinema.github.io/)], [[Demo](https://www.youtube.com/shorts/H7O-Ku_lqPA)] <br>

>>>>>>> Stashed changes
- **Make Pixels Dance: High-Dynamic Video Generation** <br>
  Team: ByteDance. <br>
  *Yan Zeng, Guoqiang Wei, Jiani Zheng, et al., Hang Li* <br>
  arXiv, 2023.11 [[Paper](https://arxiv.org/abs/2311.10982)], [[PDF](https://arxiv.org/pdf/2311.10982.pdf)], [[Home Page](https://makepixelsdance.github.io/)], [[[Demo(video)](https://www.youtube.com/watch?v=QERmPmCg9aQ)] <br>
- **Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning** <br>
  Team: Meta. <br>
  *Rohit Girdhar, Mannat Singh, Andrew Brown, et al., Ishan Misra* <br>
  arXiv, 2023.11 [[Paper](https://arxiv.org/abs/2311.10709)], [[PDF](https://arxiv.org/pdf/2311.10709.pdf)], [[Home Page](https://emu-video.metademolab.com/)], [[[Demo(live)](https://emu-video.metademolab.com/#/demo)] <br>
- **VideoCrafter1: Open Diffusion Models for High-Quality Video Generation** <br>
  Team: Tencent AI Lab. <br>
  *Haoxin Chen, Menghan Xia, Yingqing He, et al., Ying Shan* <br>
  arXiv, 2023.10 [[Paper](https://arxiv.org/abs/2310.19512)], [[PDF](https://arxiv.org/pdf/2310.19512.pdf)], [[Code](https://github.com/AILab-CVC/VideoCrafter)], [[Home Page](https://ailab-cvc.github.io/videocrafter2/)] <br>
- **SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction** <br>
  Team: Shanghai Artificial Intelligence Laboratory. <br>
  *Xinyuan Chen, Yaohui Wang, Lingjun Zhang, et al., Ziwei Liu* <br>
  arXiv, 2023.10 [[Paper](https://arxiv.org/abs/2310.20700)], [[PDF](https://arxiv.org/pdf/2310.20700.pdf)], [[Code](https://github.com/Vchitect/SEINE)], [[Home Page](https://vchitect.github.io/SEINE-project/)] <br>
- **DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors** <br>
  Team: The Chinese University of Hong Kong. <br>
  *Jinbo Xing, Menghan Xia, Yong Zhang, et al., Ying Shan* <br>
  arXiv, 2023.10 [[Paper](https://arxiv.org/abs/2310.12190)], [[PDF](https://arxiv.org/pdf/2310.12190.pdf)], [[Code](https://github.com/Doubiiu/DynamiCrafter)], [[Pretrained Model](https://github.com/Doubiiu/DynamiCrafter)], [[Home Page](https://doubiiu.github.io/projects/DynamiCrafter/)], [[Demo(live)](https://huggingface.co/spaces/Doubiiu/DynamiCrafter)], [[Demo(video)](https://www.youtube.com/watch?v=0NfmIsNAg-g)] <br>
- **LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation** <br>
  Team: Nankai University, MEGVII Technology. <br>
  *Ruiqi Wu, Liangyu Chen, Tong Yang, et al., Xiangyu Zhang* <br>
  arXiv, 2023.10 [[Paper](https://arxiv.org/abs/2310.10769)], [[PDF](https://arxiv.org/pdf/2310.10769.pdf)], [[Code](https://github.com/RQ-Wu/LAMP)], [[Pretrained Model](https://github.com/RQ-Wu/LAMP)], [[Home Page](https://rq-wu.github.io/projects/LAMP/)] <br>
- **VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning** <br>
  Team: UNC Chapel Hill. <br>
  *Han Lin, Abhay Zala, Jaemin Cho, Mohit Bansal* <br>
  arXiv, 2023.09 [[Paper](https://arxiv.org/abs/2309.15091)], [[PDF](https://arxiv.org/pdf/2309.15091.pdf)], [[Code](https://github.com/HL-hanlin/VideoDirectorGPT)] <br>
- **VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation** <br>
  Team: Baidu Inc. <br>
  *Xin Li, Wenqing Chu, Ye Wu, et al., Jingdong Wang* <br>
  arXiv, 2023.09 [[Paper](https://arxiv.org/abs/2309.00398)], [[PDF](https://arxiv.org/pdf/2309.00398.pdf)], [[Home Page](https://videogen.github.io/VideoGen/)] <br>
- **LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models** <br>
  Team: Shanghai Artificial Intelligence Laboratory. <br>
  *Yaohui Wang, Xinyuan Chen, Xin Ma, et al., Ziwei Liu* <br>
  arXiv, 2023.09 [[Paper](https://arxiv.org/abs/2309.15103)], [[PDF](https://arxiv.org/pdf/2309.15103.pdf)], [[Code](https://github.com/Vchitect/LaVie)], [[Home Page](https://vchitect.github.io/LaVie-project/)] <br>
- **Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation** <br>
  Team: Huawei. <br>
  *Jiaxi Gu, Shicong Wang, Haoyu Zhao, et al., Hang Xu* <br>
  arXiv, 2023.09 [[Paper](https://arxiv.org/abs/2309.03549)], [[PDF](https://arxiv.org/pdf/2309.03549.pdf)], [[Code](https://github.com/anonymous0x233/ReuseAndDiffuse)], [[Home Page](https://anonymous0x233.github.io/ReuseAndDiffuse/)] <br>
- **Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator** <br>
  Team: School of Information Science and Technology, ShanghaiTech University. <br>
  *Hanzhuo Huang, Yufan Feng, Cheng Shi, et al., Sibei Yang* <br>
  **NeurIPS'24**, arxiv, 2023.9[[Paper](https://arxiv.org/abs/2309.14494)], [[PDF](https://arxiv.org/pdf/2309.14494.pdf)], [[Home Page](https://github.com/showlab/Tune-A-Video)] <br>
- **Show-1: Marrying pixel and latent diffusion models for text-to-video generation.** <br>
  Team: Show Lab, National University of Singapor <br>
  *David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, et al., Mike Zheng Shou* <br>
  arxiv, 2023.09 [[Paper](https://arxiv.org/abs/2309.15818)], [[PDF](https://arxiv.org/pdf/2309.15818.pdf)], [[Home Page](https://showlab.github.io/Show-1/)],[[Code](https://github.com/showlab/Show-1)], [[Pretrained Model](https://huggingface.co/spaces/showlab/Show-1)] <br>
- **GLOBER: Coherent Non-autoregressive Video Generation via GLOBal Guided Video DecodER** <br>
  Team: Institute of Automation, Chinese Academy of Sciences (CASIA). <br>
  *Mingzhen Sun, Weining Wang, Zihan Qin, et al., Jing Liu* <br>
  NeurIPS'23, arXiv, 2023.09 [[Paper](https://arxiv.org/abs/2309.13274)], [[PDF](https://arxiv.org/pdf/2309.13274.pdf)], [[Code](https://github.com/iva-mzsun/GLOBER)], [[Home Page](https://iva-mzsun.github.io/GLOBER)], [[[Demo(video)](https://iva-mzsun.github.io/GLOBER)] <br>
- **SimDA: Simple Diffusion Adapter for Efficient Video Generation** <br>
  Team: Fudan University, Microsoft. <br>
  *Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, Yu-Gang Jiang* <br>
  arXiv, 2023.08 [[Paper](https://arxiv.org/abs/2308.09710)], [[PDF](https://arxiv.org/pdf/2308.09710.pdf)], [[Code (Coming)](https://github.com/ChenHsing/SimDA)], [[Home Page](https://chenhsing.github.io/SimDA/)] <br>
- **Empowering Dynamics-aware Text-to-Video Diffusion with Large Language Models** <br>
  Team: National University of Singapore. <br>
  *Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Tat-Seng Chua* <br>
  arXiv, 2023.08 [[Paper](https://arxiv.org/abs/2308.13812)], [[PDF](https://arxiv.org/pdf/2308.13812.pdf)], [[Code](https://github.com/scofield7419/Dysen-VDM)] <br>
- **ModelScope Text-to-Video Technical Report** <br>
  Team: Alibaba Group. <br>
  *Jiuniu Wang, Hangjie Yuan, Dayou Chen, et al., Shiwei Zhang* <br>
  arXiv, 2023.08 [[Paper](https://arxiv.org/abs/2308.06571)], [[PDF](https://arxiv.org/pdf/2308.06571.pdf)], [[Code](https://github.com/modelscope/modelscope)], [[Home Page](https://modelscope.cn/models/iic/text-to-video-synthesis/summary)], [[[Demo(live)](https://huggingface.co/spaces/ali-vilab/modelscope-text-to-video-synthesis)] <br>
- **Dual-Stream Diffusion Net for Text-to-Video Generation** <br>
  Team: Nanjing University of Science and Technology. <br>
  *Binhui Liu, Xin Liu, Anbo Dai, et al., Jian Yang* <br>
  arXiv, 2023.08 [[Paper](https://arxiv.org/abs/2308.08316)], [[PDF](https://arxiv.org/pdf/2308.08316.pdf)] <br>
- **AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning** <br>
  Team: The Chinese University of Hong Kong. <br>
  *Yuwei Guo, Ceyuan Yang, Anyi Rao, et al., Bo Dai* <br>
  **ICLR'24 (spotlight)**, arXiv, 2023.07 [[Paper](https://arxiv.org/abs/2307.04725)], [[PDF](https://arxiv.org/pdf/2307.04725.pdf)], [[Code](https://github.com/guoyww/AnimateDiff)], [[Pretrained Model](https://github.com/guoyww/AnimateDiff)], [[Home Page](https://animatediff.github.io/)] <br>
- **InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation** <br>
  Team: Shanghai AI Laboratory. <br>
  *Yi Wang, Yinan He, Yizhuo Li, et al., Yu Qiao* <br>
  ICLR'24, arXiv, 2023.07 [[Paper](https://arxiv.org/abs/2307.06942)], [[PDF](https://arxiv.org/pdf/2307.06942.pdf)], [[Code](https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid)], [[Pretrained Model](https://huggingface.co/OpenGVLab/ViCLIP)] <br>
- **Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation** <br>
  Team: HKUST. <br>
  *Yingqing He, Menghan Xia, Haoxin Chen, et al., Qifeng Chen* <br>
  arXiv, 2023.07 [[Paper](https://arxiv.org/abs/2307.06940)], [[PDF](https://arxiv.org/pdf/2307.06940.pdf)], [[Code](https://github.com/AILab-CVC/Animate-A-Story)], [[Home Page](https://ailab-cvc.github.io/Animate-A-Story/)], [[[Demo(video)](https://ailab-cvc.github.io/Animate-A-Story/)] <br>
- **Probabilistic Adaptation of Text-to-Video Models** <br>
  Team: Google, UC Berkeley. <br>
  *Mengjiao Yang, Yilun Du, Bo Dai, et al., Pieter Abbeel* <br>
  arXiv, 2023.06 [[Paper](https://arxiv.org/abs/2306.01872)], [[PDF](https://arxiv.org/pdf/2306.01872.pdf)], [[Home Page](https://video-adapter.github.io/video-adapter/)] <br>
- **ED-T2V: An Efficient Training Framework for Diffusion-based Text-to-Video Generation** <br>
  Team: School of Artificial Intelligence, University of Chinese Academy of Sciences. <br>
  *Jiawei Liu, Weining Wang, Wei Liu, Qian He, Jing Liu* <br>
  IJCNN'23, 2023.06 [[Paper](https://ieeexplore.ieee.org/abstract/document/10191565)], [[PDF](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10191565)] <br>
- **Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance** <br>
  Team: CUHK. <br>
  *Jinbo Xing, Menghan Xia, Yuxin Liu, et al., Tien-Tsin Wong* <br>
  arXiv, 2023.06 [[Paper](https://arxiv.org/abs/2306.00943)], [[PDF](https://arxiv.org/pdf/2306.00943.pdf)], [[Code](https://github.com/AILab-CVC/Make-Your-Video)], [[Pretrained Model](https://huggingface.co/Doubiiu/Make-Your-Video/blob/main/model.ckpt)], [[Home Page](https://doubiiu.github.io/projects/Make-Your-Video/)] <br>
- **VideoComposer: Compositional Video Synthesis with Motion Controllability** <br>
  Team: Alibaba Group. <br>
  *Xiang Wang, Hangjie Yuan, Shiwei Zhang, et al., Jingren Zhou* <br>
  NeurIPS'23, arXiv, 2023.06 [[Paper](https://arxiv.org/abs/2306.02018)], [[PDF](https://arxiv.org/pdf/2306.02018.pdf)], [[Code](https://github.com/ali-vilab/videocomposer)], [[Pretrained Model](https://www.modelscope.cn/models/iic/VideoComposer/summary)], [[Home Page](https://videocomposer.github.io/)] <br>
- **VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation** <br>
  Team: University of Chinese Academy of Sciences (UCAS), Alibaba Group. <br>
  *Zhengxiong Luo, Dayou Chen, Yingya Zhang, et al., Tieniu Tan* <br>
  CVPR'23, arXiv, 2023.06 [[Paper](https://arxiv.org/abs/2303.08320)], [[PDF](https://arxiv.org/pdf/2303.08320.pdf)]<br>
- **Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models** <br>
  Team: University of Maryland. <br>
  *Songwei Ge, Seungjun Nah, Guilin Liu, et al., Yogesh Balaji* <br>
  ICCV'23, arXiv, 2023.05 [[Paper](https://arxiv.org/abs/2305.10474)], [[PDF](https://arxiv.org/pdf/2305.10474.pdf)], [[Home Page](https://research.nvidia.com/labs/dir/pyoco/)] <br>
- **VideoPoet: A Large Language Model for Zero-Shot Video Generation** <br>
  Team: Google Research <br>
  *Dan Kondratyuk, Lijun Yu, Xiuye Gu, et al., Lu Jiang* <br>
  arxiv, 2023.05 [[Paper](https://arxiv.org/abs/2305.10874)], [[PDF](https://arxiv.org/pdf/2305.10874.pdf)], [[Home Page](https://sites.research.google/videopoet/)], [[Blog](https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html)] <br>
- **VideoDreamer: Customized Multi-Subject Text-to-Video Generation with Disen-Mix Finetuning** <br>
  Team: Tsinghua University, Beijing Film Academy <br>
  *Hong Chen, Xin Wang, Guanning Zeng, et al., WenwuZhu* <br>
  arxiv, 2023.05 [[Paper](https://arxiv.org/abs/2311.00990)], [[PDF](https://arxiv.org/pdf/2311.00990.pdf)], [[Code](https://github.com/videodreamer23/videodreamer23.github.io)], [[Home Page](https://videodreamer23.github.io/)] <br>
- **Text2Performer: Text-Driven Human Video Generation** <br>
  Team: Nanyang Technological University <br>
  *Yuming Jiang, Shuai Yang, Tong Liang Koh, et al., Ziwei Liu* <br>
  arxiv, 2023.04 [[Paper](https://arxiv.org/abs/2304.08483)], [[PDF](https://arxiv.org/pdf/2304.08483.pdf)], [[Code](https://github.com/yumingj/Text2Performer)], [[Home Page](https://yumingj.github.io/projects/Text2Performer.html)], [[[Demo(video)](https://www.youtube.com/watch?v=YwhaJUk_qo0)] <br>
- **Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation** <br>
  Team: University of Rochester, Meta. <br>
  *Jie An, Songyang Zhang, Harry Yang, et al., Xi Yin* <br>
  arXiv, 2023.04 [[Paper](https://arxiv.org/abs/2304.08477)], [[PDF](https://arxiv.org/pdf/2304.08477.pdf)], [[Home Page](https://latent-shift.github.io/)] <br>
- **Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models** <br>
  Team: NVIDIA. <br>
  *Andreas Blattmann, Robin Rombach, Huan Ling, et al., Karsten Kreis* <br>
  CVPR'23, arXiv, 2023.04 [[Paper](https://arxiv.org/abs/2304.08818)], [[PDF](https://arxiv.org/pdf/2304.08818.pdf)], [[Home Page](https://research.nvidia.com/labs/toronto-ai/VideoLDM/)] <br>
- **NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation** <br>
  Team: University of Science and Technology of China, Microsoft. <br>
  *Shengming Yin, Chenfei Wu, Huan Yang, et al. , Nan Duan* <br>
  arXiv, 2023.03 [[Paper](https://arxiv.org/abs/2303.12346)], [[PDF](https://arxiv.org/pdf/2303.12346.pdf)], [[Home Page](https://msra-nuwa.azurewebsites.net/#/NUWAXL)] <br>
- **Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators** <br>
  Team: Picsart AI Resarch (PAIR). <br>
  *Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, et al., Humphrey Shi* <br>
  arXiv, 2023.03 [[Paper](https://arxiv.org/abs/2303.13439)], [[PDF](https://arxiv.org/pdf/2303.13439.pdf)], [[Code](https://github.com/Picsart-AI-Research/Text2Video-Zero)], [[Home Page](https://text2video-zero.github.io/)], [[Demo(live)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)], [[Demo(video)](https://www.dropbox.com/s/uv90mi2z598olsq/Text2Video-Zero.MP4?dl=0)] <br>
- **Structure and Content-Guided Video Synthesis with Diffusion Models** <br>
  Team: Runway <br>
  *Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis* <br>
  ICCV'23, arXiv, 2023.02 [[Paper](https://arxiv.org/abs/2302.03011)], [[PDF](https://arxiv.org/pdf/2302.03011.pdf)], [[Home Page](https://research.runwayml.com/gen1)] <br>
- **MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation** <br>
  Team: Renmin University of China, Peking University, Microsoft Research  <br>
  *Ludan Ruan, Yiyang Ma, Huan Yang, et al., Baining Guo* <br>
  CVPR'23, arXiv, 2022.12 [[Paper](https://arxiv.org/abs/2212.09478)], [[PDF](https://arxiv.org/pdf/2212.09478.pdf)], [[Code](https://github.com/researchmm/MM-Diffusion?tab=readme-ov-file)] <br>
- **MagicVideo: Efficient Video Generation With Latent Diffusion Models** <br>
  Team: ByteDance Inc. <br>
  *Daquan Zhou, Weimin Wang, Hanshu Yan, et al., Jiashi Feng* <br>
  arXiv, 2022.11 [[Paper](https://arxiv.org/abs/2211.11018)], [[PDF](https://arxiv.org/pdf/2211.11018.pdf)], [[Home Page](https://magicvideo.github.io/)] <br>
- **Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation** <br>
  Team: Show Lab, National University of Singapore. <br>
  *Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Mike Zheng Shou et al* <br>
  **ICCV'23**, arxiv, 2022.12[[Paper](https://arxiv.org/abs/2212.11565)], [[PDF](https://arxiv.org/pdf/2212.11565.pdf)], [[Code](https://github.com/showlab/Tune-A-Video)], [[Pretrained Model](https://huggingface.co/Tune-A-Video-library)]  <br>
- **Latent Video Diffusion Models for High-Fidelity Long Video Generation** `Long Video` <br>
  Team: HKUST, Tencent AI Lab. <br>
  *Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, Qifeng Chen* <br>
  arXiv, 2022.10 [[Paper](https://arxiv.org/abs/2211.13221)], [[PDF](https://arxiv.org/pdf/2211.13221.pdf)], [[Code](https://github.com/YingqingHe/LVDM)], [[Home Page](https://yingqinghe.github.io/LVDM/)] <br>
- **Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation** <br>
  Team: UC Santa Barbara, Meta. <br>
  *Tsu-Jui Fu, Licheng Yu, Ning Zhang, et al., Sean Bell* <br>
  CVPR'23, arXiv, 2022.11 [[Paper](https://arxiv.org/abs/2211.12824)], [[PDF](https://arxiv.org/pdf/2211.12824.pdf)]<br>
- **Phenaki: Variable Length Video Generation From Open Domain Textual Description** <br>
  Team: Google. <br>
  *Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, et al., Dumitru Erhan* <br>
  ICLR'23, arXiv, 2022.10 [[Paper](https://arxiv.org/abs/2210.02399)], [[PDF](https://arxiv.org/pdf/2210.02399.pdf)], [[Home Page](https://sites.research.google/phenaki/)] <br>
- **Imagen Video: High Definition Video Generation with Diffusion Models** <br>
  Team: Google. <br>
  *Jonathan Ho, William Chan, Chitwan Saharia, et al., Tim Salimans* <br>
  arXiv, 2022.10 [[Paper](https://arxiv.org/abs/2210.02303)], [[PDF](https://arxiv.org/pdf/2210.02303.pdf)], [[Home Page](https://imagen.research.google/video/)] <br>
- **StoryDALL-E: Adapting Pretrained Text-to-Image Transformers for Story Continuation** `Story Visualization` <br>
  Team: UNC Chapel Hill. <br>
  *Adyasha Maharana, Darryl Hannan, Mohit Bansal* <br>
  ECCV'22, arxiv, 2022.09 [[Paper](https://arxiv.org/abs/2209.06192)], [[PDF](https://arxiv.org/pdf/2209.06192.pdf)], [[Code](https://github.com/adymaharana/storydalle)], [[[Demo(live)](https://replicate.com/adymaharana/story-dalle)] <br>
- **Make-A-Video: Text-to-Video Generation without Text-Video Data** <br>
  Team: Meta AI. <br>
  *Uriel Singer, Adam Polyak, Thomas Hayes, et al., Yaniv Taigman* <br>
  arxiv, 2022.09 [[Paper](https://arxiv.org/abs/2209.14792)], [[PDF](https://arxiv.org/pdf/2209.14792.pdf)], [[Code](https://github.com/SooLab/Free-Bloom)]<br>
- **Word-Level Fine-Grained Story Visualization** `Story Visualization` <br>
  Team: University of Oxford. <br>
  *Bowen Li, Thomas Lukasiewicz* <br>
  ECCV'22, arxiv, 2022.08 [[Paper](https://arxiv.org/abs/2208.02341)], [[PDF](https://arxiv.org/pdf/2208.02341.pdf)], [[Code](https://github.com/mrlibw/Word-Level-Story-Visualization)], [[Pretrained Model](https://github.com/mrlibw/Word-Level-Story-Visualization)]<br>
- **CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers** <br>
  Team: Tsinghua University. <br>
  *Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang* <br>
  ICLR'23, arXiv, 2022.05 [[Paper](https://arxiv.org/abs/2205.15868)], [[PDF](https://arxiv.org/pdf/2205.15868.pdf)], [[Code](https://github.com/THUDM/CogVideo)], [[Home Page](https://models.aminer.cn/cogvideo/)], [[[Demo(video)](https://huggingface.co/spaces/THUDM/CogVideo)] <br>
- **Video Diffusion Models** `text-conditioned` <br>
  Team: Google. <br>
  *Jonathan Ho, Tim Salimans, Alexey Gritsenko, et al., David J. Fleet* <br>
  arXiv, 2022.04 [[Paper](https://arxiv.org/abs/2204.03458)], [[PDF](https://arxiv.org/pdf/2204.03458.pdf)], [[Home Page](https://video-diffusion.github.io/)] <br>
- **NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis** `Long Video` <br>
  Team: Microsoft. <br>
  *Chenfei Wu, Jian Liang, Xiaowei Hu, et al., Nan Duan* <br>
  NeurIPS'22, arXiv, 2022.02 [[Paper](https://arxiv.org/abs/2207.09814)], [[PDF](https://arxiv.org/pdf/2207.09814.pdf)], [[Code](https://github.com/microsoft/NUWA)], [[Home Page](https://nuwa-infinity.microsoft.com/#/)] <br>
- **NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion** <br>
  Team: Microsoft. <br>
  *Chenfei Wu, Jian Liang, Lei Ji, et al., Nan Duan* <br>
  ECCV'22, arXiv, 2021.11 [[Paper](https://arxiv.org/abs/2111.12417)], [[PDF](https://arxiv.org/pdf/2111.12417.pdf)], [[Code](https://github.com/microsoft/NUWA)]<br>
- **GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions** <br>
  Team: Microsoft, Duke University. <br>
  *Chenfei Wu, Lun Huang, Qianxi Zhang, et al., Nan Duan* <br>
  arXiv, 2021.04 [[Paper](https://arxiv.org/abs/2104.14806)], [[PDF](https://arxiv.org/pdf/2104.14806.pdf)] <br>

## <a name="image_to_video"></a> Image-to-Video

## <a name="video_to_video"></a> Video-to-Video
- **Long video generation with time-agnostic vqgan and time-sensitive transformer** <br>
  Team: Meta AI. <br>
  *Songwei Ge, Thomas Hayes, Harry Yang, et al., Devi Parikh* <br>
  **ECCV'22** arXiv, 2022.4 [[Paper](https://arxiv.org/abs/2204.03638)], [[PDF](https://arxiv.org/pdf/2204.03638.pdf)], [[Home Page](https://songweige.github.io/projects/tats/)], [[Code](https://github.com/SongweiGe/TATS)] <br>



----
## Acknowledgement
- [Awesome Text-to-Video Generation](https://github.com/feifeiobama/Awesome-Text-to-Video-Generation)



## Citation
If you find this repository useful, please consider citing this list:
```
@misc{rui2023videogenerationlist,
    title = {Awesome-Video-Generation},
    author = {Rui Sun, Yumin Zhang},
    journal = {GitHub repository},
    url = {https://github.com/soraw-ai/Awesome-Video-Generation},
    year = {2024},
}
```

## References

